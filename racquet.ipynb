{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098a72f1",
   "metadata": {},
   "source": [
    "### Which Racquet Is It? A Deep Learning Project to Classify Badminton, Tennis, Squash & Pickleball\n",
    "\n",
    "This project demonstrates how to build a custom image classifier using the [**fastai**](https://www.fast.ai/) library, which is a high-level API built on top of the popular deep learning framework PyTorch.\n",
    "\n",
    "The goal is to teach a computer to recognize whether a racquet in a photo is for badminton, tennis, squash, or pickleball. It sounds simple, but these racquets can look quite similar — making it a fun and challenging problem to solve with deep learning.\n",
    "\n",
    "This end-to-end notebook walks through:\n",
    "- **Data collection** (including automated image scraping),\n",
    "- **Model training and tuning** using `resnet34`,\n",
    "- **Interpreting loss and error metrics** the fastai way,\n",
    "- **Testing on unseen images** with manual labeling,\n",
    "- And finally, **saving the model** for future use.\n",
    "\n",
    "The goal isn't just to build a working classifier — it's to **understand the process**, reason through decisions and lay the foundation for more complex computer vision projects down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d820b86",
   "metadata": {},
   "source": [
    "### Setup Instructions\n",
    "\n",
    "Run the following commands in your terminal to install the necessary dependencies:\n",
    "\n",
    "`pip install fastai duckduckgo_search`\n",
    "\n",
    "- **fastai** is a high-level deep learning library built on top of PyTorch, which we will use to train and evaluate our racquet image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d7bba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai is installed.\n"
     ]
    }
   ],
   "source": [
    "# Check for fastai dependencies\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from fastai.vision.all import *\n",
    "    print(\"fastai is installed.\")\n",
    "except ImportError:\n",
    "    print(\"fastai not found. Please install it via: pip install fastai\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b986ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define racquet categories\n",
    "# These names will be passed in the image search request and used to create folders for the images\n",
    "sports = [\"tennis racquet\", \"badminton racquet\", \"squash racquet\", \"pickleball racquet\"]\n",
    "\n",
    "# Base image directory\n",
    "base_dir = Path(\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca0237",
   "metadata": {},
   "source": [
    "### Create folders where images will be downloaded\n",
    "\n",
    "Note: Run the below cell only once, else you may delete all the downloaded images. You will have to download them all again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Clean up and create folders\n",
    "for item in sports:\n",
    "    sport_folder = item.split()[0].lower()  # Get 'tennis' from 'tennis racquet'\n",
    "    folder = base_dir / sport_folder\n",
    "    if folder.exists():\n",
    "        shutil.rmtree(folder)\n",
    "        print(f\"Removed existing folder: {folder.resolve()}\")\n",
    "        \n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created folder: {folder.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65fd61",
   "metadata": {},
   "source": [
    "### Downloading Racquet Images\n",
    "\n",
    "In the next step, we will download 300 images for each racquet category using the `duckduckgo_search` library. Although the images are ultimately sourced from Bing, we're using the DuckDuckGo interface to bypass the bot protection, rate limiting, and API key restrictions that come with direct access to Bing or Google.\n",
    "\n",
    "DuckDuckGo itself leverages Bing under the hood for image search results (reference: [Hacker News](https://news.ycombinator.com/item?id=23458202)). When you search for images on DuckDuckGo in a browser, the content is silently fetched from Bing, often proxied through DDG to enhance user privacy.\n",
    "\n",
    "This approach is fine atleast for our small scale and non-commercial project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "447e1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests\n",
    "from pathlib import Path\n",
    "from duckduckgo_search import DDGS\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from PIL import ImageOps\n",
    "\n",
    "MAX_SIZE = 400 #pixel size\n",
    "DELAY = 0.5 #seconds\n",
    "\n",
    "# Validate image bytes (not corrupted)\n",
    "def is_valid_image(img_bytes):\n",
    "    try:\n",
    "        img = Image.open(BytesIO(img_bytes))\n",
    "        img.verify()  # Check corruption\n",
    "\n",
    "        # Re-open the image to access dimensions and file type\n",
    "        # This is necessary because verify() doesn't load the image data\n",
    "        # Refer: https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.verify\n",
    "\n",
    "        img = Image.open(BytesIO(img_bytes))  \n",
    "\n",
    "        if img.width < 200 or img.height < 200:\n",
    "            print(f\"Image too small: {img.width}x{img.height}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Main download function\n",
    "def download_images_bing(query, folder_path, max_images):\n",
    "    print(f\"\\n~~~ Searching for: {query}\")\n",
    "    count = 0\n",
    "\n",
    "    with DDGS() as ddgs:\n",
    "        results = ddgs.images(query, max_results=max_images)\n",
    "        for result in results:\n",
    "            url = result.get(\"image\")\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                img_bytes = response.content\n",
    "\n",
    "                if not is_valid_image(img_bytes):\n",
    "                    continue  \n",
    "\n",
    "                \n",
    "                img = Image.open(BytesIO(img_bytes))\n",
    "                file_ext = img.format.lower()\n",
    "                if file_ext not in [\"jpeg\", \"jpg\", \"png\"]:\n",
    "                    print(f\"Unsupported image format: {file_ext}\")\n",
    "                    continue\n",
    "\n",
    "                #resize the images greater than MAX_SIZE to MAX_SIZE     \n",
    "                img = Image.open(BytesIO(img_bytes))       \n",
    "                if img.width > MAX_SIZE or img.height > MAX_SIZE:\n",
    "                    print(f\"Resizing image: {img.width}x{img.height}\")\n",
    "                    img = ImageOps.contain(img, (MAX_SIZE, MAX_SIZE))  # Maintains aspect ratio                    \n",
    "                    img_bytes_io = BytesIO()\n",
    "                    img.save(img_bytes_io, format=file_ext)\n",
    "                    img_bytes = img_bytes_io.getvalue()    \n",
    "                             \n",
    "                filename = f\"{query.replace(' ', '_')}_{count:03d}.{file_ext}\"\n",
    "                filepath = Path(folder_path) / filename\n",
    "\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    f.write(img_bytes)\n",
    "\n",
    "                count += 1\n",
    "                print(f\"Saved: {filepath}\")\n",
    "\n",
    "                time.sleep(DELAY)  # preventive measure for possible rate limiting on ddg\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "    print(f\"Finished downloading {count} images for '{query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21958919",
   "metadata": {},
   "source": [
    "In the above code along with downloading and validating images, we also resize images greater than 400 pixels (shorter side), while maintaining aspect ratio. This is done because:\n",
    "\n",
    "- Large images consume significantly more GPU, RAM, and disk space.\n",
    "- They slow down data loading and training.\n",
    "- For image classification tasks, a 400px resolution is typically sufficient and strikes a good balance between accuracy and efficiency.\n",
    "\n",
    "**Note** : In production datasets, we should also apply data augmentation techniques such as random cropping, flipping, rotation, brightness adjustments etc to some images randomly. This helps the model generalize better and become robust to variations in real-world inputs.\n",
    "\n",
    "However, for this small-scale project, we’re intentionally skipping augmentations to keep the workflow focused and easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8efbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images for each racquet category\n",
    "MAX_IMAGES = 300\n",
    "\n",
    "for sport in sports:\n",
    "    folder = Path(\"images\") / sport.split()[0].lower()\n",
    "    download_images_bing(sport, folder, MAX_IMAGES)\n",
    "\n",
    "# Verify image downloads\n",
    "for sport in sports:\n",
    "    folder = Path(\"images\") / sport.split()[0].lower()\n",
    "    \n",
    "    # Count all image files in the folder\n",
    "    file_count = len(list(folder.glob(\"*.*\")))\n",
    "\n",
    "    if file_count == 0:\n",
    "        print(f\"No images found in {folder}.\")\n",
    "    else:\n",
    "        print(f\"{file_count} images found in {folder}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ffcf8",
   "metadata": {},
   "source": [
    "Each category was supposed to download 300 images, but the actual counts are:\n",
    "\n",
    "- **Tennis**: 260 images  \n",
    "- **Badminton**: 191 images  \n",
    "- **Squash**: 176 images  \n",
    "- **Pickleball**: 272 images  \n",
    "\n",
    "Some images were skipped due to:\n",
    "\n",
    "- Unsupported formats like `.webp`  \n",
    "- Dimensions smaller than 200×200 pixels  \n",
    "- Access issues (e.g., 403 Forbidden)  \n",
    "- Corrupted or invalid files  \n",
    "\n",
    "Our data is downloaded and validated!\n",
    "\n",
    "**Note**: In production systems, image scraping, downloading, validation and preprocessing are never done within a training notebook or script. These tasks are treated as part of the data ingestion pipeline and are typically handled as separate jobs or services.\n",
    "\n",
    "### Let's proceed to train our model.\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7369643",
   "metadata": {},
   "source": [
    "**We'll first define a `DataBlock`.**\n",
    "\n",
    "A `DataBlock` is a high level API in `fastai` for building datasets and `DataLoaders`. It allows us to define how to get our input items, how to label them, how to split the data and what transforms to apply.\n",
    "\n",
    "You can refer to the official documentation here:  https://docs.fast.ai/data.block.html#DataBlock\n",
    "\n",
    "Below is the configuration we'll use for this project. Each line will be explained after the code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344d6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock), \n",
    "    get_items=get_image_files, \n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    get_y=parent_label,\n",
    "    item_tfms=[Resize(192, method='pad')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ed774",
   "metadata": {},
   "source": [
    "Explainaition of parameters in the code above:\n",
    "\n",
    "- `blocks`: The inputs to our model are **images**, and the outputs are **categories** (in our case, \"badminton\",\"squash\" etc).\n",
    "- `get_items`: A function to retrieve raw items — in our case: image file paths.\n",
    "- `splitter`: A function that returns training and validation indices. Here we use a random 80/20 split with a fixed seed for reproducibility.\n",
    "- `get_y`: A function to extract the label from each item — we’re using the parent folder name as the label.\n",
    "- `item_tfms`: Transformations applied to each item — here, resizing images to 192×192 by padding it. Padding maintains the aspect ratio.\n",
    "\n",
    "OK, so now we have defined the blueprint:\n",
    "\n",
    "- What type of inputs/targets to expect\n",
    "- How to get them\n",
    "- How to split the dataset\n",
    "- What transforms to apply\n",
    "\n",
    "But no actual data is loaded or processed at this point.\n",
    "\n",
    "**Create the `dataloader`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c91513",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = db.dataloaders(base_dir, bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89328d",
   "metadata": {},
   "source": [
    "What happened in the above cell?\n",
    "\n",
    "The blueprint that we created in the previous step is applied to the actual data present under `base_dir` (in our case it is the `images/` folder).\n",
    "\n",
    "The `dataloaders()` method scans the /images directory, retrieves all image files that match the blueprint, splits them into training and validation sets, applies any item transforms (like resizing), and finally batches the data.\n",
    "   \n",
    "**Batch Size (`bs=64`)**: This specifies that the DataLoaders object should create batches of 64 images. Each batch is a single, collective unit that’s fed into the model during training. Using a moderate batch size like 64 helps balance memory usage and training speed.\n",
    "\n",
    "The returned `dls` object is an instance of the `DataLoaders` class. It encapsulates:\n",
    "- **`dls.train`**: A DataLoader for the training set.\n",
    "- **`dls.valid`**: A DataLoader for the validation set.\n",
    "   \n",
    "These sub-DataLoaders are built on top of PyTorch's DataLoader but with additional fastai functionalities. Read the official doc here: https://docs.fast.ai/data.load.html\n",
    "\n",
    "\n",
    "This step has finalized our data ingestion pipeline. The created DataLoaders object (`dls`) serves as the interface between our dataset and our model during training.\n",
    "\n",
    "### Training Our Model\n",
    "\n",
    "We will begin by fine-tuning an existing, well-established computer vision model. **Fine-tuning** is the process of adapting a model that has already been trained on a large, general-purpose dataset to a new, more specific task. This saves time and cost for us.\n",
    "\n",
    "For this project, we will be using `ResNet18`, a lightweight yet powerful convolutional neural network pretrained on the ImageNet dataset. It offers a great balance between speed and accuracy, making it ideal for rapid experimentation and limited compute environments. By fine-tuning ResNet18, we can adapt its learned representations to accurately classify images across our four racquet categories.\n",
    "\n",
    "Here we go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2024e6",
   "metadata": {},
   "source": [
    "To train our model, we will use the `vision_learner` API from the [fastai vision module](https://docs.fast.ai/vision.learner.html), which is specifically designed to streamline [transfer learning](https://aws.amazon.com/what-is/transfer-learning/) for computer vision tasks. This API encapsulates everything needed to set up a `Learner` - a core abstraction in fastai that binds together a model,  dataloader (which we created above) and a loss function. You can read more about the `Learner` class [here](https://docs.fast.ai/learner.html#learner).\n",
    "\n",
    "`vision_learner` simplifies the process of leveraging pretrained models (such as ResNet18) for classification. It automatically handles the necessary setup for transfer learning, including proper initialization of the model's final layers to suit our dataset.\n",
    "\n",
    "Once the learner is configured, we invoke `fine_tune(3)`, which trains the model for 3 epochs. This method first freezes the pretrained base to train only the new classification head, and then gradually unfreezes and fine-tunes the entire model. Official doc of fine_tune ia available [here](https://docs.fast.ai/callback.schedule.html#learner.fine_tune)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa988071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.721788</td>\n",
       "      <td>0.769392</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.716825</td>\n",
       "      <td>0.562591</td>\n",
       "      <td>0.212290</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.523383</td>\n",
       "      <td>0.567538</td>\n",
       "      <td>0.206704</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.400680</td>\n",
       "      <td>0.560361</td>\n",
       "      <td>0.195531</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "22519.95s - thread._ident is None in _get_related_thread!\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/commando/python_envs/learn_ml/lib/python3.12/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=error_rate)\n",
    "learn.fine_tune(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d03e8",
   "metadata": {},
   "source": [
    "### Training Results\n",
    "\n",
    "The model was trained locally on my laptop which is a standard alptop with no dedicated GPU.\n",
    "\n",
    "- **CPU**: Intel i7 13th Gen\n",
    "- **Memory**: 16 GB  \n",
    "- **GPU**: Integrated Intel Graphics\n",
    "\n",
    "Despite lacking a dedicated GPU, training was completed in a reasonable time of around **3 minutes** for 3 epochs.\n",
    "\n",
    "Here are the training results:\n",
    "\n",
    "#### Initial Phase (frozen base layer)\n",
    "\n",
    "| epoch | train_loss | valid_loss | error_rate | time  |\n",
    "|-------|------------|------------|------------|-------|\n",
    "| 0     | 1.823223   | 0.750006   | 0.240223   | 00:20 |\n",
    "\n",
    "#### Fine-Tuning Phase (unfrozen model)\n",
    "\n",
    "| epoch | train_loss | valid_loss | error_rate | time  |\n",
    "|-------|------------|------------|------------|-------|\n",
    "| 0     | 0.663238   | 0.580100   | 0.201117   | 00:53 |\n",
    "| 1     | 0.510640   | 0.544243   | 0.173184   | 00:51 |\n",
    "| 2     | 0.372500   | 0.517990   | 0.150838   | 00:52 |\n",
    "\n",
    "As seen, the **error rate steadily decreased** with each epoch, indicating the model is learning to distinguish racquet types better over time. The **final error rate of ~15%** is quite decent for a first pass, especially considering the small dataset used by us.\n",
    "\n",
    "Before we start analyzing our results more deeply, it’s important to understand the two key components commonly found in modern deep learning workflows: the **base model** and the **custom classifier**.\n",
    "\n",
    "- The **base model** (also called the *pretrained model*, *feature extractor* or sometimes the *backbone*) is typically a convolutional neural network (eg ResNet18) that has already been trained on a large and diverse dataset (eg ImageNet). Its job is to extract general visual features (like edges, textures, and patterns) that are useful across many tasks.\n",
    "\n",
    "- The **custom classifier** (also referred to as the *head* or *task-specific classifier*) is a set of new layers added on top of the base model. These layers are trained specifically for our problem - in this case, classifying images of racquets into categories.\n",
    "\n",
    "With this structure in mind, let's break down how our model was trained in two distinct phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf1a07",
   "metadata": {},
   "source": [
    "The training was performed in two distinct phases:\n",
    "\n",
    "### **Initial Phase (Frozen Base Model)**\n",
    "\n",
    "- **Pretrained Base Model (ResNet18)**:  \n",
    "  We began with ResNet18, a convolutional neural network pretrained on ImageNet. This model has already learned to identify general visual patterns—such as edges, textures, and shapes—that are widely applicable across image classification tasks.\n",
    "\n",
    "- **Frozen Weights**:  \n",
    "  In this stage, the base model's weights are **frozen**, meaning they are **not updated during training**. This preserves the valuable feature extraction capabilities the model has learned from the large and diverse ImageNet dataset.\n",
    "\n",
    "- **Custom Classifier (Task-Specific Layers)**:  \n",
    "  On top of this frozen base model, a **custom classifier** was added by fastai’s `vision_learner` function. This classifier is a set of fully connected layers specifically designed to map the extracted features to our target classes—different types of racquets in our case.\n",
    "\n",
    "  - **In this phase**, only the *custom classifier* is trained. It learns how to interpret the high-level features produced by the base model and map them to the correct racquet class (tennis, badminton, squash or pickleball).\n",
    "  \n",
    "  - **Why This Matters**:  \n",
    "    - It allows for quick adaptation to our domain-specific data with minimal training effort.\n",
    "    - It ensures that the general visual understanding built into the base model is not disturbed, giving us a solid foundation without requiring retraining from scratch.\n",
    "\n",
    "\n",
    "#### Inspecting the Model\n",
    "\n",
    "To view the details of the model you can use-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model     #shows the complete architecture.\n",
    "learn.model[0]  #shows the base layer (ResNet18).\n",
    "learn.model[1]  #shows the head (custom classification layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a511695",
   "metadata": {},
   "source": [
    "Once our custom classifier (head) has learned to interpret the features extracted by the base model, we move to the next phase.\n",
    "\n",
    "### **Fine-Tuning Phase**\n",
    "\n",
    "#### **Unfreezing the Base Model**\n",
    "In this phase, we **unfreeze** the layers of the base model (ResNet18). This means all the convolutional layers—which were previously frozen to preserve their pretrained knowledge are now allowed to **update their weights** during training.\n",
    "\n",
    "- **Why unfreeze?**  \n",
    "  Because the base model was originally trained on a generic dataset (ImageNet), and now it’s time to **adapt those generic features to our specific task**: recognizing different types of racquets.\n",
    "  \n",
    "  For instance, ImageNet may have taught the model to recognize general features like curves, grips or mesh, but our racquet dataset may need a bit more tuning to differentiate between, say, a badminton racquet and a tennis racquet.\n",
    "\n",
    "Now, both the **base model** and the **custom classifier** are trained together:\n",
    "\n",
    "- The **classifier continues** to improve its ability to make task-specific predictions.\n",
    "- The **base model begins to adjust** its filters to extract slightly more specialized features tailored to racquet identification.\n",
    "\n",
    "#### **Key Benefits**\n",
    "- This phase allows for **deeper adaptation** to our racquet dataset. This turns out to be helpful if our dataset is visually different from ImageNet.\n",
    "- It helps the model **refine feature extraction** for better accuracy especially for edge cases.\n",
    "\n",
    "This two-phase approach is the basis of `transfer learning` as it leverages existing knowledge of ResNet18 and quickly adapting to our specific task (classifying racquet images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61c91c",
   "metadata": {},
   "source": [
    "### Detailed Explanation of Result Table Headers\n",
    "\n",
    "Now let’s understand the meaning of each column in our training results and interpret the data. \n",
    "\n",
    "Lets revisit the result of Phase 1\n",
    "| epoch | train_loss | valid_loss | error_rate | time  |\n",
    "|-------|------------|------------|------------|-------|\n",
    "| 0     | 1.823223   | 0.750006   | 0.240223   | 00:20 |\n",
    "\n",
    "Here’s a detailed analysis of each column and what the numbers mean:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7124d6",
   "metadata": {},
   "source": [
    "#### 1. **epoch**\n",
    "\n",
    "An **epoch** is one full pass over the entire training dataset. Multiple epochs allow the model to iteratively adjust its internal weights to better fit the data. So, if you train for 3 epochs, the model will see each training sample 3 times — potentially improving its predictions each time.\n",
    "\n",
    "If you remember, couple of cells above, while creating the dataloader we had configured the batch size to 64.\n",
    "\n",
    "`dls = db.dataloaders(base_dir, bs=64)`\n",
    "\n",
    "Let’s say we have a total of **800 images** for all racquets combined. We split this dataset into 80% for training and 20% for validation. That gives us **640 training images**.\n",
    "\n",
    "Given a batch size of 64, the 640 training images are divided into **10 batches**. The model is trained on each of these batches sequentially during one epoch. For each batch, it:\n",
    "- performs a forward pass to make predictions,\n",
    "- compares predictions with actual labels to compute **training loss**,\n",
    "- performs backpropagation and weight updates.\n",
    "\n",
    "After all 10 batches are processed, that completes **1 epoch**. The **training loss for the epoch** is the average of the individual batch losses.\n",
    "\n",
    "Once training for that epoch is complete, the model is evaluated on the **validation dataset (160 images)**:\n",
    "- It makes predictions on the validation set **without updating the weights**.\n",
    "- From this, we get the **validation loss** and **error rate**.\n",
    "\n",
    "In short:\n",
    "- **Epoch** = One complete pass over the **training data only**\n",
    "- **Validation** = Happens **after each epoch**, using the held-out **validation set** to assess generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d765d",
   "metadata": {},
   "source": [
    "#### 2. **train_loss** (Training Loss)\n",
    "\n",
    "During the training phase, the model learns by adjusting its internal parameters — the weights of the neural network — to minimize errors in its predictions. **Training loss** is the key metric that tells us *how far off* the model’s predictions were from the actual labels **on the training data**.\n",
    "\n",
    "For each batch, the model does the following:\n",
    "1. **Forward Pass** – It makes predictions for all 64 images in the batch.\n",
    "2. **Loss Calculation** – It compares these predictions with the actual labels using a loss function (*cross-entropy* in our case).\n",
    "3. **Backpropagation** – Based on this loss, it computes gradients and updates the weights to reduce the error.\n",
    "\n",
    "This gives us one training loss of this particular batch.\n",
    "\n",
    "Once all 10 batches (1 epoch) are processed:\n",
    "- The training losses from each batch are **averaged** to produce the **epoch-level training loss**.\n",
    "- This value gives us a sense of how well the model is doing *on the data it is actively learning from*.\n",
    "\n",
    "A lower training loss over time typically indicates that the model is learning.\n",
    "\n",
    "In our racquet classification project, the `vision_learner` function from fastai automatically selects an appropriate loss function based on the task. Since we are solving a multi-class classification problem fastai uses **[CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)**.\n",
    "\n",
    "You can confirm this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a67207cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlattenedLoss of CrossEntropyLoss()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d4ae9",
   "metadata": {},
   "source": [
    "#### 3. **valid_loss** (Validation Loss)\n",
    "\n",
    "While the **training loss** tells us how well the model is performing on the training data, the **validation loss** is our best indicator of how well the model is likely to perform on unseen data (like production environment).\n",
    "\n",
    "Once the model completes one epoch, it’s time to evaluate how well it generalizes. This is where the **validation set** comes in — the 20% portion of the data we had held back and not used during training.\n",
    "\n",
    "Here’s what happens step-by-step:\n",
    "\n",
    "- The model makes predictions on the **entire validation set**.\n",
    "- It **compares those predictions with the actual labels** (just like it does with the training data).\n",
    "- Then it calculates the **average loss** over all validation samples using the same loss function (in our case, CrossEntropyLoss).\n",
    "- This average is the **validation loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148382e0",
   "metadata": {},
   "source": [
    "#### 4. **error_rate**\n",
    "\n",
    "**Error Rate** tells us what percentage of predictions made by the model were incorrect. This gives us a more **human-readable performance measure**.\n",
    "\n",
    "Here is how it is calculated-\n",
    "\n",
    "After an epoch completes:\n",
    "- The model is run on the **validation dataset**.\n",
    "- For each image, the model outputs **predicted probabilities** for all classes (racquet types).\n",
    "- The class with the **highest probability** is selected as the model's prediction.\n",
    "- This predicted label is compared against the **actual label**.\n",
    "\n",
    "If they match : Correct.  \n",
    "If they don't : Incorrect.\n",
    "\n",
    "The **Error Rate** is then calculated as:\n",
    "\n",
    "\n",
    "> Error Rate = (Number of Incorrect Predictions)/(Total Number of Validation Samples)\n",
    "\n",
    "\n",
    "You will often see some models reporting **accuracy** — which is simply:\n",
    "\n",
    "> Accuracy = 1 - Error Rate\n",
    "\n",
    "So suppose in our example, Error Rate is **0.24** (~24%), Accuracy will be 1-0.24 = **0.76** (76%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0c93e",
   "metadata": {},
   "source": [
    "#### 5. **time**\n",
    "\n",
    "  The elapsed time to complete one epoch.\n",
    "  This metric helps gauge how quickly the model is training. It can be influenced by factors like dataset size, model complexity and hardware (e.g., CPU vs. GPU).\n",
    "\n",
    "\n",
    "#### Summary\n",
    "\n",
    "| **Header**     | **Data Used**       | **Purpose**                                                                                      | **Is Lower Better?**      |\n",
    "|----------------|---------------------|--------------------------------------------------------------------------------------------------|---------------------------|\n",
    "| **epoch**      | —                   | Indicates how many full passes the model has made over the training dataset.                    | —                         |\n",
    "| **train_loss** | Training set        | Measures how well the model fits the training data; used to guide weight updates during training.| Yes                    |\n",
    "| **valid_loss** | Validation set      | Evaluates how well the model generalizes to unseen data; key for detecting overfitting.          | Yes                    |\n",
    "| **error_rate** | Validation set      | Proportion of incorrect predictions on validation data (1 - accuracy).                           | Yes                    |\n",
    "| **time**       | Whole epoch         | Duration taken to complete one full epoch (training + validation).                              | —                         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9063fd96",
   "metadata": {},
   "source": [
    "### **Training Loss vs. Error Rate**\n",
    "\n",
    "A common confusion that some people may have (I certainly had in the beginning) is: **what’s the difference between validation loss and error rate?**  \n",
    "Though both are evaluated on the validation set, they measure very different things. Let’s break down the differences in the table below:\n",
    "\n",
    "\n",
    "| Aspect               | **Validation Loss**                                                                 | **Error Rate**                                                                 |\n",
    "|----------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|\n",
    "| **Definition**        | A numerical value from the loss function (e.g., cross-entropy)                      | The fraction of incorrect predictions                                          |\n",
    "| **What it Measures** | How well the model's **probability distribution** matches the true labels           | Whether the model's **top predicted class** is correct or not                 |\n",
    "| **Type**             | **Continuous** — can range across real values (e.g., 0.543)                         | **Discrete** — typically between 0 and 1 (e.g., 0.17 means 17% wrong)         |\n",
    "| **Sensitivity**      | Sensitive to **confidence** in correct predictions                                   | Only considers **right vs wrong**, regardless of confidence                   |\n",
    "| **Output Basis**     | Calculated from all predicted **probabilities**                                      | Calculated from final **class labels** after argmax                          |\n",
    "| **Interpretability** | More nuanced but harder to interpret directly                                        | Very interpretable — “X% predictions were wrong”                             |\n",
    "| **Goal**             | Minimize it to improve model **confidence and accuracy**                             | Minimize it to reduce outright **classification errors**                      |\n",
    "| **Use Case**         | Guides **training and optimization** of the model                                    | Helps judge **real-world prediction performance**                             |\n",
    "\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Suppose for an image of a **tennis racquet**, our model predicts the following probabilities:\n",
    "\n",
    "- **Tennis:** 0.55  \n",
    "- **Badminton:** 0.40  \n",
    "- **Squash:** 0.03  \n",
    "- **Pickleball:** 0.02  \n",
    "\n",
    "Here is how to interpret it:\n",
    "\n",
    "- **Prediction is correct**, because **tennis** has the highest probability and matches the true label.  \n",
    "- However, the **confidence** (55%) is not very high—this means the model wasn’t very *sure*.  \n",
    "- **Validation loss** (e.g., cross-entropy loss) will still be **moderately high**, because it penalizes low confidence even when the prediction is correct.  \n",
    "- **Error rate = 0**, because the top predicted class is correct.\n",
    "\n",
    "To Summarize:\n",
    "\n",
    "- Use **validation loss** for model optimization and fine-tuning.\n",
    "- Use **error rate** for a high-level, human-readable performance metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd108913",
   "metadata": {},
   "source": [
    "### Interpreting Train Loss, Valid Loss and Error Rate\n",
    "\n",
    "[Jeremy Howard](https://x.com/jeremyphoward), co-founder of fastai and a respected authority in AI, emphasizes that overfitting in deep learning is widely misunderstood. Contrary to popular belief, a lower **training loss** compared to **validation loss** is not a sign of failure. It is expected and reflects a properly trained model. He says that overfitting is rare in modern deep learning and requires deliberate effort to induce, such as disabling safeguards like data augmentation, dropout or weight decay.  \n",
    "\n",
    "The true indicator of overfitting isn’t the loss gap but a **rising validation error rate** - the point where the model’s prediction accuracy on unseen data deteriorates despite improving training performance. He clarifies that we should focus on **error rates** (e.g., misclassifications) rather than obsessing over loss values. As long as error rate improves, longer training is beneficial, even if validation loss increases.  \n",
    "\n",
    "Summary:  \n",
    "- **Error rate** (not loss) determines overfitting.  \n",
    "- Modern models generalize well unless stripped of regularization tools.  \n",
    "- Continue training until error rate plateaus or start increasing.  \n",
    "\n",
    "[Original source](https://x.com/jeremyphoward/status/1073361458048561154)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9131f4",
   "metadata": {},
   "source": [
    "### Testing the Trained Model on Unseen Images\n",
    "\n",
    "Our model is now fully trained, and it's time to evaluate its performance on unseen images of racquets.\n",
    "\n",
    "To do this, I downloaded a handful of racquet images from the internet and placed them in a separate folder named **`test_images`**, which resides at the same level as this Jupyter notebook file.\n",
    "\n",
    "To make evaluation easier, I manually renamed each file by prefixing the filename with the **first letter of the correct category**:\n",
    "\n",
    "eg:\n",
    "- `t_1.jpg` → Tennis racquet  \n",
    "- `b_3.png` → Badminton racquet  \n",
    "- `s_4.jpeg` → Squash racquet  \n",
    "- `p_2.jpg` → Pickleball racquet  \n",
    "\n",
    "This convention allows us to **automatically extract the correct label** for each image and compare it with the model’s prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205884f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'badminton'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the test images path\n",
    "test_images_folder = Path(\"test_images\")\n",
    "\n",
    "prefix_to_label = {\n",
    "    'b': 'badminton',\n",
    "    't': 'tennis',\n",
    "    's': 'squash',\n",
    "    'p': 'pickleball'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997a44f",
   "metadata": {},
   "source": [
    "With the setup in place, we are now ready to test our model against the unseen racquet images.\n",
    "\n",
    "We iterate over each image in the `test_images` folder and use our trained model to make predictions. At the same time, we infer the correct label from the filename prefix (b, t, s, or p) using a simple dictionary mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec5934c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Skipping :: Unknown prefix 'r' in filename 'r_3.png'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * p_2.jpeg | Actual: pickleball | Predicted: pickleball | Probabilities: [b: 0.0 , p: 99.91 , s: 0.01 , t: 0.08] | Correct\n",
      "\n",
      " -- Skipping ::  Unsupported file ext: p_football.webp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * b_broken_1.jpg | Actual: badminton | Predicted: badminton | Probabilities: [b: 80.58 , p: 0.04 , s: 1.09 , t: 18.3] | Correct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * t_2.jpeg | Actual: tennis | Predicted: tennis | Probabilities: [b: 0.36 , p: 7.9 , s: 0.12 , t: 91.63] | Correct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * p_1.png | Actual: pickleball | Predicted: pickleball | Probabilities: [b: 0.0 , p: 100.0 , s: 0.0 , t: 0.0] | Correct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * t_1.png | Actual: tennis | Predicted: squash | Probabilities: [b: 2.24 , p: 0.0 , s: 58.06 , t: 39.7] | xxx Incorrect xxx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * s_2.png | Actual: squash | Predicted: squash | Probabilities: [b: 33.58 , p: 0.0 , s: 63.36 , t: 3.06] | Correct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * b_1.png | Actual: badminton | Predicted: badminton | Probabilities: [b: 99.75 , p: 0.01 , s: 0.03 , t: 0.2] | Correct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * p_22.png | Actual: pickleball | Predicted: pickleball | Probabilities: [b: 0.0 , p: 99.81 , s: 0.01 , t: 0.18] | Correct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * b_broken_ch_2.png | Actual: badminton | Predicted: badminton | Probabilities: [b: 88.69 , p: 0.2 , s: 7.98 , t: 3.14] | Correct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * t_3.jpeg | Actual: tennis | Predicted: badminton | Probabilities: [b: 73.21 , p: 0.03 , s: 0.89 , t: 25.87] | xxx Incorrect xxx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * s_1.png | Actual: squash | Predicted: squash | Probabilities: [b: 0.01 , p: 0.0 , s: 99.96 , t: 0.03] | Correct\n",
      "\n",
      "\n",
      "Total images: 11. Correct: 9.\n",
      "Accuracy: 81.82%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for img_file in test_images_folder.ls():\n",
    "    if img_file.suffix.lower() in ['.jpg', '.png', '.jpeg']:\n",
    "        first_letter = img_file.name[0].lower() #p/b/t/s\n",
    "        \n",
    "        if first_letter not in prefix_to_label:\n",
    "            print(f\" -- Skipping :: Unknown prefix '{first_letter}' in filename '{img_file.name}'.\")\n",
    "            continue\n",
    "\n",
    "        correct_label = prefix_to_label.get(first_letter)\n",
    "        \n",
    "        pred_label, pred_idx, probs = learn.predict(img_file)\n",
    "\n",
    "        #get the probabilities for each category\n",
    "       \n",
    "        prob_str = \"\"\n",
    "        for i in range(len(probs)):\n",
    "            prob_percentage = round(probs[i].item()*100, 2)\n",
    "            prob_category = learn.dls.vocab[i]\n",
    "            prob_str = f\"{prob_str} , {prob_category[0]}: {prob_percentage}\"\n",
    "                \n",
    "        prob_str = prob_str[3:]       \n",
    "        \n",
    "        \n",
    "        result = \"xxx Incorrect xxx\"\n",
    "        if pred_label == correct_label:\n",
    "            result = \"Correct\"   \n",
    "            correct += 1\n",
    "\n",
    "        total += 1         \n",
    "        \n",
    "        print(f\" * {img_file.name} | Actual: {correct_label} | Predicted: {pred_label} | Probabilities: [{prob_str}] | {result}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"\\n -- Skipping ::  Unsupported file ext: {img_file.name}\")\n",
    "\n",
    "print(f\"\\n\\nTotal images: {total}. Correct: {correct}.\")\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62aafc4",
   "metadata": {},
   "source": [
    "### Result Evaluation\n",
    "\n",
    "In the above code I tested the performance of our model on a set of 13 images. \n",
    "\n",
    "We skipped 2 files having:\n",
    "- Unknown prefix (`r_3.png`)\n",
    "- Unsupported file format (`p_football.webp`)\n",
    "\n",
    "The model made predictions on 11 valid images. For each one, we displayed:\n",
    "- Actual category (inferred from filename)\n",
    "- Predicted category\n",
    "- Prediction confidence scores for each category\n",
    "- Whether the prediction was **correct** or **incorrect**\n",
    "\n",
    "Here’s a snapshot of how our model performed:\n",
    "\n",
    "- **Correct predictions**: 9  \n",
    "- **Incorrect predictions**: 2  \n",
    "- **Accuracy**: 81.82%\n",
    "\n",
    "The 2 incorrect predictions were:\n",
    "\n",
    "- `t_1.png` (actual: **tennis**) → predicted as **squash**.  \n",
    "- `t_3.jpeg` (actual: **tennis**) → predicted as **badminton**. \n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Despite a **small test set** (170-270 images per category), our model showed promising generalization. It classified images with pretty decent accuracy and minimal tuning. With an accuracy of **81.82%**, the model is doing well but there's a scope for improvement. Here’s how to push it further:\n",
    "\n",
    "#### - Add More and Better Training Data\n",
    "- Increase dataset size\n",
    "- **Add variety**: different angles, lighting, backgrounds etc. This helps the model generalize better to real-world cases.\n",
    "\n",
    "#### - Use Data Augmentation\n",
    "- Apply transforms like **rotation, zoom, lighting changes** and **flipping**.\n",
    "- Simulates real-world distortions and improves robustness.\n",
    "- Fastai provides several APIs to makes this easy.\n",
    "\n",
    "#### - Fine-Tune the Whole Model\n",
    "The model can be further improved using advanced techniques like **unfreezing the backbone**, **discriminative learning rates** and **progressive resizing**. These approaches allow deeper layers to adapt to task-specific patterns more effectively. However, these are broader topics and go beyond the scope of this project. We may explore them more deeply in future projects.\n",
    "\n",
    "\n",
    "This wraps up the training and evaluation phase. Our model is now ready to be integrated into real-world applications or further optimized for production use.\n",
    "\n",
    "Last but not the least, let's see how we can save our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa347db5",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "\n",
    "Saving the model allows us to:\n",
    "- Preserve all the learned weights and architecture.\n",
    "- Avoid retraining from scratch every time.\n",
    "- Quickly reload the model for inference or further fine-tuning.\n",
    "\n",
    "In fastai, it’s straightforward:\n",
    "\n",
    "`learn.export('racquet_classifier.pkl')`\n",
    "\n",
    "\n",
    "This creates a file named *racquet_classifier.pkl*, which contains everything needed to make predictions later — including the model architecture, trained weights and class mappings.\n",
    "\n",
    "By default, the model will be saved in the same directory as your notebook. You can also specify a different path if needed:\n",
    "\n",
    "`learn.export('/path/to/folder/racquet_classifier.pkl')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06c9cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "learn.export('racquet_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5b348",
   "metadata": {},
   "source": [
    "To load the model later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b4aa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_reborn = load_learner('racquet_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a159f8",
   "metadata": {},
   "source": [
    "Now *learn_reborn* is ready to classify racquets — no retraining required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094117c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'badminton'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets try to predict the category of a badminton image using the reborn model\n",
    "pred_label, pred_idx, probs = learn_reborn.predict('./test_images/b_1.png')\n",
    "pred_label #it successfully predicts the category as badminton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb68b9",
   "metadata": {},
   "source": [
    "### Thank You!\n",
    "\n",
    "I hope this small project helped you understand not just how to train a deep learning model, but also how to interpret its performance and put it to practical use. If you found even a small part of it useful, my efforts are successful.\n",
    "\n",
    "Till next time, happy learning! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
